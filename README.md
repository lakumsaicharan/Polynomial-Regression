# ğŸ“Š Polynomial Regression

A machine learning implementation of Polynomial Regression algorithm using Python and scikit-learn. This project demonstrates how to capture non-linear relationships between variables by transforming features into polynomial terms.

## ğŸ“ Description

Polynomial Regression is an extension of linear regression that models the relationship between the independent variable x and dependent variable y as an nth degree polynomial. It's particularly useful when the relationship between variables is non-linear and can be approximated by a polynomial function.

## âœ¨ Key Features

- ğŸ“Š Non-linear regression modeling
- ğŸ”¢ Flexible degree selection (quadratic, cubic, etc.)
- ğŸ“¶ Captures curvilinear relationships
- ğŸ› ï¸ Feature transformation with PolynomialFeatures
- ğŸ’» Implemented using scikit-learn
- ğŸ“ˆ Visualization of polynomial curves
- âš¡ Fast training and prediction
- ğŸ¯ Better fit than linear regression for curved data

## ğŸ› ï¸ Technologies Used

- **Python 3.x** - Programming language
- **scikit-learn** - Machine learning library
- **NumPy** - Numerical computing
- **pandas** - Data manipulation
- **Matplotlib** - Data visualization
- **Jupyter Notebook** - Interactive development

## ğŸ“ Project Structure

```
Polynomial-Regression/
â”œâ”€â”€ Polynomial Regression    # Jupyter notebook with implementation
â”œâ”€â”€ LICENSE                  # MIT License
â””â”€â”€ README.md                # Project documentation
```

## ğŸš€ Installation

1. **Clone the repository:**
   ```bash
   git clone https://github.com/lakumsaicharan/Polynomial-Regression.git
   cd Polynomial-Regression
   ```

2. **Install required dependencies:**
   ```bash
   pip install numpy pandas matplotlib scikit-learn jupyter
   ```

3. **Launch Jupyter Notebook:**
   ```bash
   jupyter notebook
   ```

## ğŸ’» Usage

1. Open the `Polynomial Regression` notebook
2. Run the cells sequentially to:
   - Load and explore the dataset
   - Transform features into polynomial terms
   - Train the Polynomial Regression model
   - Make predictions
   - Visualize the polynomial curve

### Basic Implementation

```python
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
import numpy as np

# Transform features to polynomial
poly_features = PolynomialFeatures(degree=2)
X_poly = poly_features.fit_transform(X)

# Create and train the model
regressor = LinearRegression()
regressor.fit(X_poly, y)

# Make predictions
y_pred = regressor.predict(poly_features.transform(X_test))
```

## ğŸ¯ How Polynomial Regression Works

### Mathematical Formula:

**For degree 2 (Quadratic):**
```
y = Î²â‚€ + Î²â‚x + Î²â‚‚xÂ²
```

**For degree 3 (Cubic):**
```
y = Î²â‚€ + Î²â‚x + Î²â‚‚xÂ² + Î²â‚ƒxÂ³
```

**General form (degree n):**
```
y = Î²â‚€ + Î²â‚x + Î²â‚‚xÂ² + ... + Î²â‚™xâ¿
```

### Feature Transformation:

For input `x = 5` with degree 3:
```
Original: [5]
Transformed: [1, 5, 25, 125]
           [xâ°, xÂ¹, xÂ², xÂ³]
```

## ğŸ”¢ Degree Selection

### Common Polynomial Degrees:

**Degree 1 (Linear):**
```python
poly = PolynomialFeatures(degree=1)  # y = Î²â‚€ + Î²â‚x
```

**Degree 2 (Quadratic):**
```python
poly = PolynomialFeatures(degree=2)  # y = Î²â‚€ + Î²â‚x + Î²â‚‚xÂ²
```

**Degree 3 (Cubic):**
```python
poly = PolynomialFeatures(degree=3)  # Captures S-shaped curves
```

**Degree 4+:**
```python
poly = PolynomialFeatures(degree=4)  # Higher flexibility, risk of overfitting
```

## ğŸ“Š Model Evaluation Metrics

```python
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error

# R-squared score
r2 = r2_score(y_test, y_pred)

# Mean Squared Error
mse = mean_squared_error(y_test, y_pred)

# Mean Absolute Error
mae = mean_absolute_error(y_test, y_pred)

# Root Mean Squared Error
rmse = np.sqrt(mse)
```

## ğŸ“ˆ Advantages

âœ… **Captures non-linear relationships**  
âœ… **More flexible than linear regression**  
âœ… **Easy to implement and understand**  
âœ… **Works well for curved data patterns**  
âœ… **No need for feature scaling**  
âœ… **Can model complex relationships** with higher degrees  
âœ… **Interpretable coefficients**

## âš ï¸ Limitations

âŒ **Prone to overfitting** with high degrees  
âŒ **Poor extrapolation** beyond training data range  
âŒ **Sensitive to outliers**  
âŒ **Computational cost increases** with degree  
âŒ **Multicollinearity** issues with high-degree polynomials  
âŒ **Not suitable for all non-linear relationships**

## ğŸ”§ Choosing the Right Degree

### Best Practices:

1. **Start with degree 2** (quadratic) for most cases
2. **Use cross-validation** to find optimal degree
3. **Plot the curves** for different degrees
4. **Watch for overfitting** with high degrees
5. **Consider domain knowledge** about the relationship

### Degree Selection Guide:

```python
from sklearn.model_selection import cross_val_score

degrees = range(1, 6)
scores = []

for degree in degrees:
    poly = PolynomialFeatures(degree=degree)
    X_poly = poly.fit_transform(X)
    model = LinearRegression()
    score = cross_val_score(model, X_poly, y, cv=5, 
                           scoring='r2').mean()
    scores.append(score)

best_degree = degrees[np.argmax(scores)]
```

## ğŸ“š Learning Objectives

This project demonstrates:

- âœ… Understanding Polynomial Regression algorithm
- âœ… Feature transformation with PolynomialFeatures
- âœ… Selecting optimal polynomial degree
- âœ… Avoiding overfitting in polynomial models
- âœ… Comparing with linear regression
- âœ… Visualizing polynomial curves
- âœ… Model evaluation and validation
- âœ… Handling non-linear relationships

## ğŸ” Visualization

### Polynomial Curve Fitting:
```python
import matplotlib.pyplot as plt
import numpy as np

# Original data
plt.scatter(X, y, color='red', label='Actual Data')

# Polynomial curve
X_plot = np.linspace(X.min(), X.max(), 100).reshape(-1, 1)
X_plot_poly = poly_features.transform(X_plot)
y_plot = regressor.predict(X_plot_poly)

plt.plot(X_plot, y_plot, color='blue', label=f'Polynomial (degree={degree})')
plt.xlabel('Feature')
plt.ylabel('Target')
plt.title('Polynomial Regression')
plt.legend()
plt.show()
```

### Comparing Different Degrees:
```python
for degree in [1, 2, 3, 4]:
    poly = PolynomialFeatures(degree=degree)
    X_poly = poly.fit_transform(X)
    model = LinearRegression().fit(X_poly, y)
    
    X_plot_poly = poly.transform(X_plot)
    y_plot = model.predict(X_plot_poly)
    
    plt.plot(X_plot, y_plot, label=f'Degree {degree}')

plt.scatter(X, y, color='red', label='Data')
plt.legend()
plt.show()
```

## ğŸ¨ Common Use Cases

- ğŸ“Š **Growth Curves:** Population, sales, revenue growth
- ğŸŒ¡ï¸ **Temperature Trends:** Seasonal variations
- ğŸ“ˆ **Stock Prices:** Short-term price movements
- âš¡ **Energy Consumption:** Load curves
- ğŸ’‰ **Dose-Response:** Medical dosage effects
- ğŸï¸ **Speed vs Fuel:** Vehicle efficiency curves

## ğŸ”„ Comparison with Other Algorithms

| Algorithm | Linearity | Interpretability | Overfitting Risk | Extrapolation |
|-----------|-----------|------------------|------------------|---------------|
| Polynomial Regression | Non-linear | High | Medium-High | Poor |
| Linear Regression | Linear | High | Low | Good |
| Decision Tree | Non-linear | High | High | Poor |
| Support Vector Regression | Non-linear | Low | Medium | Medium |
| Random Forest | Non-linear | Medium | Low | Poor |

## ğŸ’¡ Overfitting Prevention

### Techniques:

1. **Regularization (Ridge/Lasso):**
```python
from sklearn.linear_model import Ridge

poly = PolynomialFeatures(degree=4)
X_poly = poly.fit_transform(X)
ridge = Ridge(alpha=1.0)
ridge.fit(X_poly, y)
```

2. **Cross-Validation:**
```python
from sklearn.model_selection import cross_val_score

scores = cross_val_score(model, X_poly, y, cv=5, scoring='r2')
print(f"Average RÂ²: {scores.mean():.3f}")
```

3. **Train-Test Split:**
```python
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)
```

## ğŸ¤ Contributing

Contributions are welcome! Feel free to:
- Report bugs or issues
- Suggest new features
- Submit pull requests
- Improve documentation
- Add visualization examples

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ‘¤ Author

**Lakum Sai Charan**
- GitHub: [@lakumsaicharan](https://github.com/lakumsaicharan)

## ğŸŒŸ Acknowledgments

- scikit-learn documentation and community
- Machine learning best practices
- #100DaysOfCode community
- Polynomial regression research

## ğŸ“š Mathematical Details

### Feature Matrix Transformation:

**Original features:**
```
X = [[xâ‚],
     [xâ‚‚],
     [xâ‚ƒ]]
```

**After PolynomialFeatures(degree=2):**
```
X_poly = [[1, xâ‚, xâ‚Â²],
          [1, xâ‚‚, xâ‚‚Â²],
          [1, xâ‚ƒ, xâ‚ƒÂ²]]
```

### Model Training:

The model learns coefficients Î² by solving:
```
min â€–y - X_polyÎ²â€–Â²
```

Using normal equation:
```
Î² = (X_polyáµ€ X_poly)â»Â¹ X_polyáµ€ y
```

## ğŸ’¡ Next Steps

- ğŸ” Implement **regularized polynomial regression** (Ridge/Lasso)
- ğŸ“Š Try **Spline Regression** for smoother curves
- ğŸ§  Explore **Generalized Additive Models** (GAM)
- ğŸ“ˆ Add **interaction features** between variables
- âš¡ Compare with **non-parametric methods** like KNN

---

*Part of my Machine Learning journey* ğŸš€
